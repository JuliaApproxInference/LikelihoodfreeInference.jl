var documenterSearchIndex = {"docs":
[{"location":"generated/toyexample/#","page":"Example: Gaussian with Given Variance","title":"Example: Gaussian with Given Variance","text":"EditURL = \"https://github.com/jbrea/LikelihoodfreeInference.jl/blob/master/docs/literate/toyexample.jl\"","category":"page"},{"location":"generated/toyexample/#Example:-Gaussian-with-Given-Variance-1","page":"Example: Gaussian with Given Variance","title":"Example: Gaussian with Given Variance","text":"","category":"section"},{"location":"generated/toyexample/#","page":"Example: Gaussian with Given Variance","title":"Example: Gaussian with Given Variance","text":"As a model we are given a univariate Gaussian distribution with unknown mean and standard deviation sigma = 1. We have one data point at 2.0. For this toy example, we can compute the posterior over the mean. But to illustrate likelihood-free inference, let us assume here that we can only sample from the model:","category":"page"},{"location":"generated/toyexample/#","page":"Example: Gaussian with Given Variance","title":"Example: Gaussian with Given Variance","text":"using LikelihoodfreeInference, Distributions, Random\nmodel(x) = randn() .+ x","category":"page"},{"location":"generated/toyexample/#","page":"Example: Gaussian with Given Variance","title":"Example: Gaussian with Given Variance","text":"LikelihoodfreeInference.jl passes parameter values as vectors to the model, even in the one-dimensional case. In our definition of the model we assume that x[1] is the mean.","category":"page"},{"location":"generated/toyexample/#Approximate-the-Posterior-1","page":"Example: Gaussian with Given Variance","title":"Approximate the Posterior","text":"","category":"section"},{"location":"generated/toyexample/#","page":"Example: Gaussian with Given Variance","title":"Example: Gaussian with Given Variance","text":"Our first goal is to find the posterior over the mean given observation and a Gaussian prior with mean 0 and standard deviation 5.","category":"page"},{"location":"generated/toyexample/#","page":"Example: Gaussian with Given Variance","title":"Example: Gaussian with Given Variance","text":"data = [2.0]\nprior = MultivariateNormal([0.], [5.])","category":"page"},{"location":"generated/toyexample/#","page":"Example: Gaussian with Given Variance","title":"Example: Gaussian with Given Variance","text":"The true posterior is a Gaussian distribution with mean 25/26*2 and standard deviation 26/25","category":"page"},{"location":"generated/toyexample/#","page":"Example: Gaussian with Given Variance","title":"Example: Gaussian with Given Variance","text":"trueposterior = pdf.(Normal.(-1:.01:5, 26/25), 25/26*2.0)\nusing Plots\nfigure = Plots.plot(-1:.01:5, trueposterior, label = \"posterior\")","category":"page"},{"location":"generated/toyexample/#","page":"Example: Gaussian with Given Variance","title":"Example: Gaussian with Given Variance","text":"Now, we will use an adaptive sequential Monte Carlo method:","category":"page"},{"location":"generated/toyexample/#","page":"Example: Gaussian with Given Variance","title":"Example: Gaussian with Given Variance","text":"smc = AdaptiveSMC(prior = prior, K = 10^4)\nresult = run!(smc, model, data, verbose = true, maxfevals = 10^6);\nnothing #hide","category":"page"},{"location":"generated/toyexample/#","page":"Example: Gaussian with Given Variance","title":"Example: Gaussian with Given Variance","text":"As a Monte Carlo Method the result is a list of particles","category":"page"},{"location":"generated/toyexample/#","page":"Example: Gaussian with Given Variance","title":"Example: Gaussian with Given Variance","text":"particles(smc)","category":"page"},{"location":"generated/toyexample/#","page":"Example: Gaussian with Given Variance","title":"Example: Gaussian with Given Variance","text":"with corresponding weights","category":"page"},{"location":"generated/toyexample/#","page":"Example: Gaussian with Given Variance","title":"Example: Gaussian with Given Variance","text":"weights(smc)","category":"page"},{"location":"generated/toyexample/#","page":"Example: Gaussian with Given Variance","title":"Example: Gaussian with Given Variance","text":"The mean of the posterior is given by weights(smc) .* particles(smc), which is computed by the mean function.","category":"page"},{"location":"generated/toyexample/#","page":"Example: Gaussian with Given Variance","title":"Example: Gaussian with Given Variance","text":"mean(smc)\n\nfigure = histogram(vcat(particles(smc)...), weights = weights(smc), normalize = true, label = \"AdaptiveSMC\")\nplot!(figure, -1:.01:5, trueposterior, label = \"posterior\")","category":"page"},{"location":"generated/toyexample/#","page":"Example: Gaussian with Given Variance","title":"Example: Gaussian with Given Variance","text":"The result above also contains these weights and particles and some additional information.","category":"page"},{"location":"generated/toyexample/#","page":"Example: Gaussian with Given Variance","title":"Example: Gaussian with Given Variance","text":"keys(result)","category":"page"},{"location":"generated/toyexample/#","page":"Example: Gaussian with Given Variance","title":"Example: Gaussian with Given Variance","text":"AdaptiveSMC reduced the epsilon parameter adaptively, as we saw in column epsilon of the run above. We can plot this sequence.","category":"page"},{"location":"generated/toyexample/#","page":"Example: Gaussian with Given Variance","title":"Example: Gaussian with Given Variance","text":"Plots.scatter(cumsum(result.n_sims)[2:end], result.epsilons,\n              yscale = :log10, ylabel = \"epsilon\", xlabel = \"number of model evaluations\")","category":"page"},{"location":"generated/toyexample/#","page":"Example: Gaussian with Given Variance","title":"Example: Gaussian with Given Variance","text":"Alternatively, we may want to use KernelABC.","category":"page"},{"location":"generated/toyexample/#","page":"Example: Gaussian with Given Variance","title":"Example: Gaussian with Given Variance","text":"kabc = KernelABC(prior = prior,\n                 kernel = Kernel(),\n                 delta = 1e-8,\n                 K = 10^4)\nresult = run!(kabc, model, data, maxfevals = 10^4)\nmean(kabc)\n\nfigure = histogram(vcat(particles(kabc)...), weights = weights(kabc),\n                   xlims = (-1, 5), bins = 100,\n                   normalize = true, label = \"KernelABC\")\nplot!(figure, -1:.01:5, trueposterior, label = \"posterior\")","category":"page"},{"location":"generated/toyexample/#Point-Estimates-1","page":"Example: Gaussian with Given Variance","title":"Point Estimates","text":"","category":"section"},{"location":"generated/toyexample/#","page":"Example: Gaussian with Given Variance","title":"Example: Gaussian with Given Variance","text":"Sometimes we just want a point estimate. We will use BayesianOptimization.jl here to minimize the default QDLoss. We know that the true maximum likelihood estimate is at mean = 25/26*2 â‰ˆ 1.923","category":"page"},{"location":"generated/toyexample/#","page":"Example: Gaussian with Given Variance","title":"Example: Gaussian with Given Variance","text":"using BayesianOptimization\np = PointEstimator(optimizer = bo([-10.], [10.]), prior = prior, K = 100)\nresult = run!(p, model, data, maxfevals = 5*10^4, verbose = false);\nresult.x","category":"page"},{"location":"generated/toyexample/#","page":"Example: Gaussian with Given Variance","title":"Example: Gaussian with Given Variance","text":"KernelRecursiveABC is an alternative method that requires often only few model evaluations in low and medium dimensional problems","category":"page"},{"location":"generated/toyexample/#","page":"Example: Gaussian with Given Variance","title":"Example: Gaussian with Given Variance","text":"k = KernelRecursiveABC(prior = prior,\n                       kernel = Kernel(),\n                       kernelx = Kernel(),\n                       delta = 1e-2,\n                       K = 100)\nresult = run!(k, model, data, maxfevals = 2*10^3)\nresult.x","category":"page"},{"location":"generated/toyexample/#","page":"Example: Gaussian with Given Variance","title":"Example: Gaussian with Given Variance","text":"","category":"page"},{"location":"generated/toyexample/#","page":"Example: Gaussian with Given Variance","title":"Example: Gaussian with Given Variance","text":"This page was generated using Literate.jl.","category":"page"},{"location":"reference/#Reference-1","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/#Approximate-Bayesian-Computation-(ABC)-1","page":"Reference","title":"Approximate Bayesian Computation (ABC)","text":"","category":"section"},{"location":"reference/#","page":"Reference","title":"Reference","text":"Modules = [LikelihoodfreeInference]\nPages = [\"smc.jl\", \"kernelabc.jl\"]","category":"page"},{"location":"reference/#LikelihoodfreeInference.AdaptiveSMC","page":"Reference","title":"LikelihoodfreeInference.AdaptiveSMC","text":"AdaptiveSMC(; alpha = 0.9, epsilon = .1, ess_min = 0.5, M = 1,\n              prior, K = 10^3, proposal = defaultproposal(prior))\n\nAdaptive Sequential Monte Carlo structure with K particles, M calls of the model per parameter value, final epsilon, decrease parameter alpha and resampling threshold ess_min. See also PMC.\n\nExample\n\nusing Distributions, LinearAlgebra\nasmc = AdaptiveSMC(prior = MultivariateNormal(zeros(2), 4I))\nmodel(theta) = vcat([.3*randn(2) .+ theta for _ in 1:2]...) # 2 i.i.d samples\ndata = [.4, -.3, .5, -.2]\nrun!(asmc, model, data)\n\nmean(asmc)\nusing StatsPlots\np = hcat(particles(asmc)...); StatsPlots.scatter(p[1, :], p[2, :])\n\nDel Moral, P., Doucet, A., and Jasra, A. (2012) An adaptive sequential Monte Carlo method for approximate Bayesian computation. Statistics and Computing, 22, 1009-1020 \n\n\n\n\n\n","category":"type"},{"location":"reference/#LikelihoodfreeInference.PMC","page":"Reference","title":"LikelihoodfreeInference.PMC","text":"PMC(; epsilon, K, prior, ess_min = 0.5, proposal = defaultproposal(prior))\n\nPopulation Monte Carlo ABC structure, with epsilon schedule, K particles, prior, resampling threshold ess_min and proposal distribution. The epsilon schedule should be an iterable, e.g. an array of numbers, EpsilonExponentialDecay or EpsilonLinearDecay. One dimensional problems should be defined in vectorized form ( see run! for an example).\n\nExample\n\npmc = PMC(epsilon = EpsilonExponentialDecay(1, .01, .9), K = 20,\n                 prior = MultivariateUniform([-1, -1], [1, 1]))\nmodel(theta) = .1 * randn(2) .+ theta\ndata = [.3, -.2]\nrun!(pmc, model, data)\n\nmean(pmc)\nparticles(pmc)\nweights(pmc)\n\nBeaumont, M. A., Cornuet, J., Marin, J., and Robert, C. P. (2009) Adaptive approximate Bayesian computation. Biometrika,96, 983-990. \n\n\n\n\n\n","category":"type"},{"location":"reference/#LikelihoodfreeInference.run!-Tuple{Any,Any,Any}","page":"Reference","title":"LikelihoodfreeInference.run!","text":"run!([rng], method, model, data;\n     callback = () -> nothing, maxfevals = Inf, verbose = true)\n\nRun method on model and data. The function callback gets evaluated after every iteration. The method stops after the first iteration that reaches more than maxfevals calls to the model.\n\nExample\n\nusing Distributions, LinearAlgebra\npmc = PMC(epsilon = [1, .5, .2, .1, .01, .001], K = 10^3,\n                 prior = TruncatedMultivariateNormal([1.], 2I,\n                                                     lower = [0], upper = [3]))\nmodel(theta) = theta * randn() .+ 1.2\ndata = [.5]\ncallback() = @show mean(pmc)\nrun!(pmc, model, data, callback = callback, maxfevals = 10^5)\n\nusing StatsBase, StatsPlots\nh = fit(Histogram, vcat(particles(pmc)...), nbins = 50)\nStatsPlots.plot(h)\n\n\n\n\n\n","category":"method"},{"location":"reference/#LikelihoodfreeInference.K2ABC-Tuple{}","page":"Reference","title":"LikelihoodfreeInference.K2ABC","text":"K2ABC(; kernel, epsilon, prior, K)\n\nCreates a K2ABC structure.\n\nExample\n\nusing Distributions\nmodel(x) = [randn() .+ x for _ in 1:3]\ndata = [[3.], [2.9], [3.3]]\nk = K2ABC(kernel = Kernel(),\n          epsilon = .1,\n          prior = MultivariateNormal([0.], [5.]),\n          K = 10^3)\nresult = run!(k, model, data)\nmean(k)\n\nPark, M., Jitkrittum, W., and Sejdinovic, D. (2016), K2-ABC: Approximate Bayesian Computation with Kernel Embeddings, Proceedings of Machine Learning Research, 51:398-407 \n\n\n\n\n\n","category":"method"},{"location":"reference/#LikelihoodfreeInference.KernelABC-Tuple{}","page":"Reference","title":"LikelihoodfreeInference.KernelABC","text":"KernelABC(; kernel, prior, K, delta)\n\nCreates a KernelABC structure.\n\nExample\n\nusing Distributions\nmodel(x) = [randn() .+ x for _ in 1:3]\ndata = [[3.], [2.9], [3.3]]\nk = KernelABC(kernel = Kernel(),\n              delta = 1e-8,\n              prior = MultivariateNormal([0.], [5.]),\n              K = 10^3)\nresult = run!(k, model, data)\nmean(k)\n\nFukumizu, K., Song, L. and Gretton, A. (2013), Kernel Bayes' Rule: Bayesian Inference with Positive Definite Kernels, Journal of Machine Learning Research, 14:3753-3783, http://jmlr.org/papers/v14/fukumizu13a.html\n\nSee also Muandet, K., Fukumizu, K., Sriperumbudur, B. and SchÃ¶lkopf, B. (2017), Kernel Mean Embedding of Distributions: A Review and Beyond, Foundations and TrendsÂ® in Machine Learning, 10:1â€“141 \n\n\n\n\n\n","category":"method"},{"location":"reference/#Approximate-Point-Estimators-1","page":"Reference","title":"Approximate Point Estimators","text":"","category":"section"},{"location":"reference/#","page":"Reference","title":"Reference","text":"For possible optimizers for PointEstimator see (Optional) Optimizers for PointEstimator.","category":"page"},{"location":"reference/#","page":"Reference","title":"Reference","text":"Modules = [LikelihoodfreeInference]\nPages = [\"pointestimator.jl\", \"krabc.jl\"]","category":"page"},{"location":"reference/#LikelihoodfreeInference.KernelLoss","page":"Reference","title":"LikelihoodfreeInference.KernelLoss","text":"KernelLoss(; loss = L, K = 50,\n             kerneltype = ModifiedGaussian,\n             gamma = .1,\n             heuristic = Nothing,\n             prior = nothing,\n             distance = euclidean,\n             kernel = Kernel(type = kerneltype,\n                             heuristic = heuristic,\n                             gamma = gamma,\n                             distance = distance))\n\nConstructs an structure, used to compare K samples generated from a model to some data with a kernel and loss function loss. The loss function has the signature L(list_of_samples, data, kernel); by default it is the negative log of the average distance between samples and data (see LikelihoodfreeInference.L). The KernelLoss can be used as a loss in a PointEstimator.\n\nThe returned structure is callable.\n\nExample\n\nmodel(x) = randn() .+ x\ndata = [2.]\nk = KernelLoss()\nk([1.], model, data)\n\nK can be an integer or a function that returns integers, to implement a schedule.\n\nExample\n\nusing Statistics\nmodel(x) = randn() .+ x\ndata = [2.]\nk = KernelLoss(K = (n) -> n > 20 ? 10^3 : 10)\nstd(k([1.], model, data, 1) for _ in 1:100)   # large std, because K = 10\nstd(k([1.], model, data, 21) for _ in 1:100)  # small std, because K = 10^3\n\nThis KernelLoss is inspired by Bertl, J., Ewing, G., Kosiol, C., and Futschik, A., (2017) Approximate maximum likelihood estimation for population genetic inference, Statistical Applications in Genetics and Molecular Biology. 16:5-6 \n\n\n\n\n\n","category":"type"},{"location":"reference/#LikelihoodfreeInference.PointEstimator","page":"Reference","title":"LikelihoodfreeInference.PointEstimator","text":"PointEstimator(optimizer, loss)\n\n\n\n\n\n","category":"type"},{"location":"reference/#LikelihoodfreeInference.PointEstimator-Tuple{}","page":"Reference","title":"LikelihoodfreeInference.PointEstimator","text":"PointEstimator(; prior = nothing, lower = nothing, upper = nothing,\n                 optimizer = nlopt(prior),\n                 losstype = QDLoss, kwargs...)\n\nCreates a PointEstimator with given prior, optimizer and loss = losstype(; prior = prior, kwargs...). If the prior is nothing a uniform prior is assumed. In this case the optimizer has to be initialized with bounds, , e.g. nlopt(lower, upper) where lower and upper are arrays.\n\nExample\n\nmodel(x) = randn() .+ x\ndata = [2.]\np = PointEstimator(lower = [-5], upper = [5], losstype = QDLoss, K = 10^3)\nres = run!(p, model, data, maxfevals = 10^5)\n\n\n\n\n\n","category":"method"},{"location":"reference/#LikelihoodfreeInference.QDLoss-Tuple{}","page":"Reference","title":"LikelihoodfreeInference.QDLoss","text":"QDLoss(; K = 50, epsilon = 2/K, prior = nothing, distance = euclidean)\n\nConstructs an structure, used to compare K samples generated from a model to some data by computing the first epsilon-quantile of the distance between samples and data. The QDLoss can be used as a loss in a PointEstimator.\n\nThe returned structure is callable.\n\nExample\n\nmodel(x) = randn() .+ x\ndata = [2.]\nq = QDLoss()\nq([1.], model, data)\n\nK can be an integer or a function that returns integers, to implement a schedule. Similarly, epsilon can be a number or a function that returns numbers.\n\nExample\n\nusing Statistics\nmodel(x) = randn() .+ x\ndata = [2.]\nq = QDLoss(K = (n) -> n > 20 ? 10^3 : 10)\nstd(q([1.], model, data, 1) for _ in 1:100)   # large std, because K = 10\nstd(q([1.], model, data, 21) for _ in 1:100)  # small std, because K = 10^3\n\n\n\n\n\n","category":"method"},{"location":"reference/#LikelihoodfreeInference.KernelRecursiveABC-Tuple{}","page":"Reference","title":"LikelihoodfreeInference.KernelRecursiveABC","text":"KernelRecursiveABC(; kernel, prior, K, delta = 1e-2,\n                     herding_options = (maxtime = 10,\n                                        method = :LD_LBFGS,\n                                        restarts = 10),\n                     kernelx = Kernel())\n\nCreates a KernelRecursiveABC structure. KernelRecursiveABC iterates a KernelABC step and a kernel herding step. kernel, prior, K and delta determine the KernelABC step. herding_options options and kernelx determine the herding step.\n\nExample\n\nusing Distributions\nmodel(x) = [randn() .+ x for _ in 1:3]\ndata = [[3.], [2.9], [3.3]]\nk = KernelRecursiveABC(kernel = Kernel(),\n                       delta = 1e-8,\n                       prior = MultivariateNormal([0.], [5.]),\n                       K = 10^2)\nresult = run!(k, model, data, maxfevals = 10^3)\nresult.x\n\nKajihara, T., Kanagawa, M., Yamazaki, K. and Fukumizu, K. (2018), Kernel Recursive ABC: Point Estimation with Intractable Likelihood, Proceedings of the 35th International Conference on Machine Learning, 80:2400-2409 \n\n\n\n\n\n","category":"method"},{"location":"reference/#LikelihoodfreeInference.L-Tuple{Any,Any,Any}","page":"Reference","title":"LikelihoodfreeInference.L","text":"L(x, y, k) = -log(mean(k(xi, y) for xi in x))\n\n\n\n\n\n","category":"method"},{"location":"reference/#Additional-Distributions-1","page":"Reference","title":"Additional Distributions","text":"","category":"section"},{"location":"reference/#","page":"Reference","title":"Reference","text":"Modules = [LikelihoodfreeInference]\nPages = [\"distributions.jl\"]","category":"page"},{"location":"reference/#LikelihoodfreeInference.MultiParticleNormal","page":"Reference","title":"LikelihoodfreeInference.MultiParticleNormal","text":"MultiParticleNormal(particles, weights;\n                    lower = nothing, upper = nothing,\n                    diagonal = false,\n                    scaling = 2, regularization = 0.)\n\nMixture of k multivariate Gaussians with means given by an array of particles (length(particles) == k), weighted by weights and with covariance matrix given by scaling * cov(particles) + regularization * I for each Gaussian, if diagonal == false; otherwise scaling * std(particles) + regularization. Sampling truncated Gaussians with diagonal covariance matrix can be more efficient. The mean of the distribution is the weighted average of the particles.\n\nIf lower and upper are not nothing the Gaussians are truncated to the box confined by the lower and upper bounds. Note that the logpdf of the truncated version is not properly normalized (see also TruncatedMultivariateNormal).\n\nExample\n\njulia> d = MultiParticleNormal([[0, 0], [2, 3], [4, 0]], [1/3, 1/3, 1/3]);\n\njulia> mean(d)\n2-element Array{Float64,1}:\n 2.0\n 1.0\n\n\n\n\n\n","category":"type"},{"location":"reference/#LikelihoodfreeInference.MultiParticleNormal-Tuple{}","page":"Reference","title":"LikelihoodfreeInference.MultiParticleNormal","text":"MultiParticleNormal(; kwargs...)\n\nReturns a anonymous function that accepts particles as input and creates a MultiParticleNormal distributions with uniform weights. It accepts the same keyword arguments kwargs... as the normal MultiParticleNormal constructor.\n\nExample\n\njulia> constructor = MultiParticleNormal(scaling = 3, lower = [-1, 0], upper = [5, 5]);\n\njulia> d = constructor([[0, 0], [2, 3], [4, 0], [5, 1]])\nMultiParticleNormal(4 particles in 2 dimensions, scaling = 3.0, regularization = 0.0, with bounds)\n\njulia> d.weights\n4-element Array{Float64,1}:\n 0.25\n 0.25\n 0.25\n 0.25\n\n\n\n\n\n","category":"method"},{"location":"reference/#LikelihoodfreeInference.MultivariateUniform","page":"Reference","title":"LikelihoodfreeInference.MultivariateUniform","text":"MultivariateUniform(lower, upper)\n\nUniform distribution in d dimensions, where d == length(lower) == length(upper).\n\nExample\n\njulia> d = MultivariateUniform([-2., -3], [4, 1])\nMultivariateUniform{Float64} in 2 dimensions\n\njulia> logpdf(d, [-3, 0])\n-Inf\n\njulia> logpdf(d, [-1, 0])\n-3.1780538303479453\n\n\n\n\n\n","category":"type"},{"location":"reference/#LikelihoodfreeInference.TruncatedMultivariateNormal","page":"Reference","title":"LikelihoodfreeInference.TruncatedMultivariateNormal","text":"TruncatedMultivariateNormal(mvnormal, lower, upper)\n\nMultivariate normal distribution mvnormal truncated to a box given by lower and upper bounds. Simple rejection sampling is implemented. IMPORTANT: logpdf is not properly normalized.\n\nExample\n\njulia> using Distributions\n\njulia> d = TruncatedMultivariateNormal(MultivariateNormal([2., 3.], .3*I), [0, 0], [4, 5]);\n\njulia> logpdf(d, [2, 3])\n-0.6339042620834094\n\njulia> logpdf(d, [2, 3]) == logpdf(d.mvnormal, [2, 3])\ntrue\n\njulia> logpdf(d, [-1, 1])\n-Inf\n\n\n\n\n\n","category":"type"},{"location":"reference/#LikelihoodfreeInference.TruncatedMultivariateNormal-Tuple{Any,Any}","page":"Reference","title":"LikelihoodfreeInference.TruncatedMultivariateNormal","text":"TruncatedMultivariateNormal(m, cov; lower, upper)\n\nConstructs a TruncatedMultivariateNormal with mean m covariance matrix cov and bounds lower and upper.\n\nExample\n\njulia> d = TruncatedMultivariateNormal([0, 0], 3I;\n                                       lower = [-1, -4], upper = [4, 5])\n\n\n\n\n\n","category":"method"},{"location":"reference/#(Optional)-Optimizers-for-PointEstimator-1","page":"Reference","title":"(Optional) Optimizers for PointEstimator","text":"","category":"section"},{"location":"reference/#","page":"Reference","title":"Reference","text":"Modules = [LikelihoodfreeInference]\nPages = [\"optimizer.jl\", \"optional_optimization.jl\"]","category":"page"},{"location":"reference/#LikelihoodfreeInference.bbo-Tuple{Any,Any}","page":"Reference","title":"LikelihoodfreeInference.bbo","text":"bbo(lower, upper; kwargs...)\nbbo(prior; kwargs...) = bbo(lower(prior), upper(prior); kwargs...)\n\nBlackBoxOptim optimizer for PointEstimator, available when using BlackBoxOptim. kwargs are passed to BlackBoxOptim.bboptimize.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LikelihoodfreeInference.bo-Tuple{Any,Any}","page":"Reference","title":"LikelihoodfreeInference.bo","text":"bo(lower, upper;\n        acquisition = BO.UpperConfidenceBound(),\n        capacity = 10^3,\n        mean = BO.GaussianProcesses.MeanConst(0.),\n        kernel = BO.GaussianProcesses.SEArd(zeros(length(lower)), 5.),\n        logNoise = 0,\n        modeloptimizer_options = NamedTuple(),\n        kwargs...)\nbo(prior; kwargs...) = bo(lower(prior), upper(prior); kwargs...)\n\nBayesianOptimization optimizer for PointEstimator, available when using BayesianOptimization. kwargs are passed to BayesianOptimization.BOpt.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LikelihoodfreeInference.cma-Tuple{Any,Any}","page":"Reference","title":"LikelihoodfreeInference.cma","text":"cma(lower, upper;\n    sigma0 = 0.25,\n    fminoptions = NamedTuple(),\n    filename = \"tmp-\",\n    kwargs...)\n cma(prior; kwargs...) = cma(lower(prior), upper(prior); kwargs...)\n\nCMA optimizer for PointEstimator, available when using PyCMA. kwargs are passed to PyCMA.cma.CMAOptions, fminoptions to PyCMA.cma.fmin.\n\n\n\n\n\n","category":"method"},{"location":"reference/#LikelihoodfreeInference.spsa-Tuple{Any,Any}","page":"Reference","title":"LikelihoodfreeInference.spsa","text":"spsa(lower, upper; kwargs...)\nspsa(prior; kwargs...) = spsa(lower(prior), upper(prior); kwargs...)\n\nSimultaneousPerturbationStochasticApproximation optimizer for PointEstimator, available when using SimultaneousPerturbationStochasticApproximation. kwargs are passed to SimultaneousPerturbationStochasticApproximation.SPSA.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Utilities-1","page":"Reference","title":"Utilities","text":"","category":"section"},{"location":"reference/#","page":"Reference","title":"Reference","text":"Modules = [LikelihoodfreeInference]\nPages = [\"utils.jl\"]","category":"page"},{"location":"reference/#LikelihoodfreeInference.EpsilonExponentialDecay","page":"Reference","title":"LikelihoodfreeInference.EpsilonExponentialDecay","text":"EpsilonExponentialDecay(init, last, decay)\n\nExponentially decreasing sequence starting at init, decaying with factor decay, ending as soon as it equals (or drops below) last.\n\nExample\n\njulia> collect(EpsilonExponentialDecay(1, .2, .5))\n4-element Array{Any,1}:\n 1.0\n 0.5\n 0.25\n 0.125\n\n\n\n\n\n","category":"type"},{"location":"reference/#LikelihoodfreeInference.EpsilonLinearDecay","page":"Reference","title":"LikelihoodfreeInference.EpsilonLinearDecay","text":"EpsilonLinearDecay(init, last, decay)\n\nLinearly decreasing sequence starting at init, decaying with step size decay, ending as soon as it equals (or drops below) last.\n\nExample\n\njulia> collect(EpsilonLinearDecay(10, 5, 2))\n4-element Array{Any,1}:\n 10.0\n  8.0\n  6.0\n  4.0\n\n\n\n\n\n","category":"type"},{"location":"reference/#LikelihoodfreeInference.defaultproposal-Tuple{Any}","page":"Reference","title":"LikelihoodfreeInference.defaultproposal","text":"defaultproposal(prior)\n\nReturns a function that takes a list of particles and returns a proposal distribution. This function is called during initialization of PMC and AdaptiveSMC.\n\n\n\n\n\n","category":"method"},{"location":"#Introduction-1","page":"Introduction","title":"Introduction","text":"","category":"section"},{"location":"#","page":"Introduction","title":"Introduction","text":"Given some measured data y0 and a potentially stochastic model model(x) that takes parameters x and returns simulated data y, LikelihoodfreeInference.jl allows to find approximate posterior distributions over x or approximate maximum likelihood (ML) and maximum a posteriori (MAP) point estimates, by runnning","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"run!(method, model, y0)","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"where method can be an Approximate Bayesian Computation (ABC) method PMC, AdaptiveSMC, K2ABC, KernelABC (subtypes(LikelihoodfreeInference.AbstractABC)) or PointEstimator, KernelRecursiveABC (subtypes(LikelihoodfreeInference.AbstractPointABC)).","category":"page"},{"location":"#Example-1","page":"Introduction","title":"Example","text":"","category":"section"},{"location":"#","page":"Introduction","title":"Introduction","text":"using LikelihoodfreeInference, StatsPlots, Distributions\nmodel(x) = randn() .+ x\ndata = [2.]\nmethod = KernelABC(delta = 1e-8,\n                   K = 10^3,\n                   kernel = Kernel(),\n                   prior = TruncatedMultivariateNormal([0.], [5.],\n                                                       lower = [-5.],\n                                                       upper = [5.]))\nresult = run!(method, model, data)\nprintln(\"Approximate posterior mean = $(mean(method))\")\nfigure = histogram(method, normalize = true, xlims = (-5, 5))\nplot!(figure, -1:.01:5, pdf.(Normal.(-1:.01:5, 26/25), 25/26*2.0))","category":"page"},{"location":"generated/blowfly/#","page":"Example: Blowfly Model","title":"Example: Blowfly Model","text":"EditURL = \"https://github.com/jbrea/LikelihoodfreeInference.jl/blob/master/docs/literate/blowfly.jl\"","category":"page"},{"location":"generated/blowfly/#Example:-Blowfly-Model-1","page":"Example: Blowfly Model","title":"Example: Blowfly Model","text":"","category":"section"},{"location":"generated/blowfly/#","page":"Example: Blowfly Model","title":"Example: Blowfly Model","text":"Likelihood-free inference for the blow-fly model was introduced by Simon N. Wood. We model here the discrete time stochastic dynamics of the size N of an adult blowfly population as given in section 1.2.3 of the supplementary information.","category":"page"},{"location":"generated/blowfly/#","page":"Example: Blowfly Model","title":"Example: Blowfly Model","text":"N_t+1 = P N_t-tauexp(-N_t-tauN_0)e_t + N_texp(-delta epsilon_t)","category":"page"},{"location":"generated/blowfly/#","page":"Example: Blowfly Model","title":"Example: Blowfly Model","text":"where e_t and epsilon_t are independent Gamma random deviates with mean 1 and variance sigma_p^2 and sigma_d^2, respectively.","category":"page"},{"location":"generated/blowfly/#","page":"Example: Blowfly Model","title":"Example: Blowfly Model","text":"using Distributions, StatsBase, LikelihoodfreeInference\nBase.@kwdef struct BlowFlyModel\n    burnin::Int = 50\n    T::Int = 1000\nend\nfunction (m::BlowFlyModel)(P, Nâ‚€, Ïƒd, Ïƒp, Ï„, Î´)\n    p1 = Gamma(1/Ïƒp^2, Ïƒp^2)\n    p2 = Gamma(1/Ïƒd^2, Ïƒd^2)\n    T = m.T + m.burnin + Ï„\n    N = fill(180., T)\n    for t in Ï„+1:T-1\n        N[t+1] = P * N[t-Ï„] * exp(-N[t-Ï„]/Nâ‚€)*rand(p1) + N[t]*exp(-Î´*rand(p2))\n    end\n    N[end-m.T+1:end]\nend","category":"page"},{"location":"generated/blowfly/#","page":"Example: Blowfly Model","title":"Example: Blowfly Model","text":"Let us plot four realizations from this model with the same parameters.","category":"page"},{"location":"generated/blowfly/#","page":"Example: Blowfly Model","title":"Example: Blowfly Model","text":"using StatsPlots\nplotly()\nm = BlowFlyModel()\nplot([plot(m(29, 260, .6, .3, 7, .2),\n           xlabel = \"t\", ylabel = \"N\", legend = false) for _ in 1:4]...,\n     layout = (2, 2))","category":"page"},{"location":"generated/blowfly/#","page":"Example: Blowfly Model","title":"Example: Blowfly Model","text":"To compare different realizations we will use histogram summary statistics. In the literature one finds also other summary statistics for this data.","category":"page"},{"location":"generated/blowfly/#","page":"Example: Blowfly Model","title":"Example: Blowfly Model","text":"summary_statistics(N) = fit(Histogram, N, 140:16:16140).weights","category":"page"},{"location":"generated/blowfly/#","page":"Example: Blowfly Model","title":"Example: Blowfly Model","text":"We will use a normal prior on log-transformed parameters.","category":"page"},{"location":"generated/blowfly/#","page":"Example: Blowfly Model","title":"Example: Blowfly Model","text":"function parameter(logparams)\n    lP, lNâ‚€, lÏƒd, lÏƒp, lÏ„, lÎ´ = logparams\n    (P = round(exp(2 + 2lP)),\n    Nâ‚€ = round(exp(4 + .5lNâ‚€)),\n    Ïƒd = exp(-.5 + lÏƒd),\n    Ïƒp = exp(-.5 + lÏƒp),\n    Ï„ = round(Int, max(1, min(500, exp(2 + lÏ„)))),\n    Î´ = exp(-1 + .4lÎ´))\nend\n(m::BlowFlyModel)(logparams) = m(parameter(logparams)...)\ntarget(m::BlowFlyModel) = [(log(29) - 2)/2,\n                           (log(260) - 4)*2,\n                           log(.6) + .5,\n                           log(.3) + .5,\n                           log(7) - 2,\n                           (log(.2) + 1)/.4]\nlower(m::BlowFlyModel) = fill(-5., 6)\nupper(m::BlowFlyModel) = fill(5., 6)\nprior = TruncatedMultivariateNormal(zeros(6), ones(6),\n                                    lower = lower(m), upper = upper(m))","category":"page"},{"location":"generated/blowfly/#","page":"Example: Blowfly Model","title":"Example: Blowfly Model","text":"Let us now generate some target data.","category":"page"},{"location":"generated/blowfly/#","page":"Example: Blowfly Model","title":"Example: Blowfly Model","text":"model = BlowFlyModel()\nx0 = target(model)\ndata = summary_statistics(model(x0))","category":"page"},{"location":"generated/blowfly/#Adaptive-SMC-1","page":"Example: Blowfly Model","title":"Adaptive SMC","text":"","category":"section"},{"location":"generated/blowfly/#","page":"Example: Blowfly Model","title":"Example: Blowfly Model","text":"smc = AdaptiveSMC(prior = prior)\nresult = run!(smc, x -> summary_statistics(model(x)), data,\n              maxfevals = 2*10^5, verbose = false)\nusing PrettyTables\npretty_table([[keys(parameter(zeros(6)))...] quantile(smc, .05) median(smc) mean(smc) x0 quantile(smc, .95)],\n             [\"names\", \"5%\", \"median\", \"mean\", \"actual\", \"95%\"],\n             formatter = ft_printf(\"%10.3f\"))","category":"page"},{"location":"generated/blowfly/#","page":"Example: Blowfly Model","title":"Example: Blowfly Model","text":"histogram(smc)","category":"page"},{"location":"generated/blowfly/#","page":"Example: Blowfly Model","title":"Example: Blowfly Model","text":"corrplot(smc)","category":"page"},{"location":"generated/blowfly/#KernelABC-1","page":"Example: Blowfly Model","title":"KernelABC","text":"","category":"section"},{"location":"generated/blowfly/#","page":"Example: Blowfly Model","title":"Example: Blowfly Model","text":"k = KernelABC(prior = prior, delta = 1e-1, K = 10^3, kernel = Kernel())\nresult = run!(k, x -> summary_statistics(model(x)), data)\npretty_table([[keys(parameter(zeros(6)))...] quantile(k, .05) median(k) mean(k) x0 quantile(k, .95)],\n             [\"names\", \"5%\", \"median\", \"mean\", \"actual\", \"95%\"],\n             formatter = ft_printf(\"%10.3f\"))","category":"page"},{"location":"generated/blowfly/#","page":"Example: Blowfly Model","title":"Example: Blowfly Model","text":"histogram(k)","category":"page"},{"location":"generated/blowfly/#Kernel-Recursive-ABC-(with-callback)-1","page":"Example: Blowfly Model","title":"Kernel Recursive ABC (with callback)","text":"","category":"section"},{"location":"generated/blowfly/#","page":"Example: Blowfly Model","title":"Example: Blowfly Model","text":"k = KernelRecursiveABC(prior = prior,\n                       K = 100,\n                       delta = 1e-3,\n                       kernel = Kernel(bandwidth = Bandwidth(heuristic = MedianHeuristic(2^3))),\n                       kernelx = Kernel());\nnothing #hide","category":"page"},{"location":"generated/blowfly/#","page":"Example: Blowfly Model","title":"Example: Blowfly Model","text":"We will use a callback here to show how the estimated parameters evolves.","category":"page"},{"location":"generated/blowfly/#","page":"Example: Blowfly Model","title":"Example: Blowfly Model","text":"using LinearAlgebra\nres_krabc = run!(k, x -> summary_statistics(model(x)), data,\n                 maxfevals = 1300,\n                 verbose = true,\n                 callback = () -> @show norm(k.theta - x0)/norm(x0))","category":"page"},{"location":"generated/blowfly/#","page":"Example: Blowfly Model","title":"Example: Blowfly Model","text":"","category":"page"},{"location":"generated/blowfly/#","page":"Example: Blowfly Model","title":"Example: Blowfly Model","text":"This page was generated using Literate.jl.","category":"page"}]
}
