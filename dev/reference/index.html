<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Reference · LikelihoodfreeInference.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">LikelihoodfreeInference.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Introduction</a></li><li><a class="tocitem" href="../generated/toyexample/">Example: Gaussian with Given Variance</a></li><li><a class="tocitem" href="../generated/blowfly/">Example: Blowfly Model</a></li><li class="is-active"><a class="tocitem" href>Reference</a><ul class="internal"><li><a class="tocitem" href="#Approximate-Bayesian-Computation-(ABC)-1"><span>Approximate Bayesian Computation (ABC)</span></a></li><li><a class="tocitem" href="#Approximate-Point-Estimators-1"><span>Approximate Point Estimators</span></a></li><li><a class="tocitem" href="#Additional-Distributions-1"><span>Additional Distributions</span></a></li><li><a class="tocitem" href="#(Optional)-Optimizers-for-PointEstimator-1"><span>(Optional) Optimizers for PointEstimator</span></a></li><li><a class="tocitem" href="#Utilities-1"><span>Utilities</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Reference</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Reference</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/jbrea/LikelihoodfreeInference.jl/blob/master/docs/src/reference.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Reference-1"><a class="docs-heading-anchor" href="#Reference-1">Reference</a><a class="docs-heading-anchor-permalink" href="#Reference-1" title="Permalink"></a></h1><h2 id="Approximate-Bayesian-Computation-(ABC)-1"><a class="docs-heading-anchor" href="#Approximate-Bayesian-Computation-(ABC)-1">Approximate Bayesian Computation (ABC)</a><a class="docs-heading-anchor-permalink" href="#Approximate-Bayesian-Computation-(ABC)-1" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LikelihoodfreeInference.AdaptiveSMC" href="#LikelihoodfreeInference.AdaptiveSMC"><code>LikelihoodfreeInference.AdaptiveSMC</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">AdaptiveSMC(; alpha = 0.9, epsilon = .1, ess_min = 0.5, M = 1,
              prior, K = 10^3, proposal = defaultproposal(prior))</code></pre><p>Adaptive Sequential Monte Carlo structure with <code>K</code> particles, <code>M</code> calls of the model per parameter value, final <code>epsilon</code>, decrease parameter <code>alpha</code> and resampling threshold <code>ess_min</code>. See also <a href="#LikelihoodfreeInference.PMC"><code>PMC</code></a>.</p><p><strong>Example</strong></p><pre><code class="language-none">using Distributions, LinearAlgebra
asmc = AdaptiveSMC(prior = MultivariateNormal(zeros(2), 4I))
model(theta) = vcat([.3*randn(2) .+ theta for _ in 1:2]...) # 2 i.i.d samples
data = [.4, -.3, .5, -.2]
run!(asmc, model, data)

mean(asmc)
using StatsPlots
p = hcat(particles(asmc)...); StatsPlots.scatter(p[1, :], p[2, :])</code></pre><p><a href="http://dx.doi.org/10.1007/s11222-011-9271-y">Del Moral, P., Doucet, A., and Jasra, A. (2012) An adaptive sequential Monte Carlo method for approximate Bayesian computation. Statistics and Computing, 22, 1009-1020 </a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbrea/LikelihoodfreeInference.jl/blob/679e229bcc3b04fe902332c2054fbb8b9908670f/src/smc.jl#L159-L185">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LikelihoodfreeInference.PMC" href="#LikelihoodfreeInference.PMC"><code>LikelihoodfreeInference.PMC</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">PMC(; epsilon, K, prior, ess_min = 0.5, proposal = defaultproposal(prior))</code></pre><p>Population Monte Carlo ABC structure, with <code>epsilon</code> schedule, <code>K</code> particles, <code>prior</code>, resampling threshold <code>ess_min</code> and <code>proposal</code> distribution. The <code>epsilon</code> schedule should be an iterable, e.g. an array of numbers, <a href="#LikelihoodfreeInference.EpsilonExponentialDecay"><code>EpsilonExponentialDecay</code></a> or <a href="#LikelihoodfreeInference.EpsilonLinearDecay"><code>EpsilonLinearDecay</code></a>. One dimensional problems should be defined in vectorized form ( see <a href="#LikelihoodfreeInference.run!-Tuple{Any,Any,Any}"><code>run!</code></a> for an example).</p><p><strong>Example</strong></p><pre><code class="language-julia-repl">pmc = PMC(epsilon = EpsilonExponentialDecay(1, .01, .9), K = 20,
                 prior = MultivariateUniform([-1, -1], [1, 1]))
model(theta) = .1 * randn(2) .+ theta
data = [.3, -.2]
run!(pmc, model, data)

mean(pmc)
particles(pmc)
weights(pmc)</code></pre><p><a href="http://dx.doi.org/10.1093/biomet/asp052">Beaumont, M. A., Cornuet, J., Marin, J., and Robert, C. P. (2009) Adaptive approximate Bayesian computation. Biometrika,96, 983-990. </a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbrea/LikelihoodfreeInference.jl/blob/679e229bcc3b04fe902332c2054fbb8b9908670f/src/smc.jl#L1-L27">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LikelihoodfreeInference.run!-Tuple{Any,Any,Any}" href="#LikelihoodfreeInference.run!-Tuple{Any,Any,Any}"><code>LikelihoodfreeInference.run!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">run!([rng], method, model, data;
     callback = () -&gt; nothing, maxfevals = Inf, verbose = true)</code></pre><p>Run <code>method</code> on <code>model</code> and <code>data</code>. The function <code>callback</code> gets evaluated after every iteration. The method stops after the first iteration that reaches more than <code>maxfevals</code> calls to the model.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl">using Distributions, LinearAlgebra
pmc = PMC(epsilon = [1, .5, .2, .1, .01, .001], K = 10^3,
                 prior = TruncatedMultivariateNormal([1.], 2I,
                                                     lower = [0], upper = [3]))
model(theta) = theta * randn() .+ 1.2
data = [.5]
callback() = @show mean(pmc)
run!(pmc, model, data, callback = callback, maxfevals = 10^5)

using StatsBase, StatsPlots
h = fit(Histogram, vcat(particles(pmc)...), nbins = 50)
StatsPlots.plot(h)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbrea/LikelihoodfreeInference.jl/blob/679e229bcc3b04fe902332c2054fbb8b9908670f/src/smc.jl#L91-L113">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LikelihoodfreeInference.K2ABC-Tuple{}" href="#LikelihoodfreeInference.K2ABC-Tuple{}"><code>LikelihoodfreeInference.K2ABC</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">K2ABC(; kernel, epsilon, prior, K)</code></pre><p>Creates a <code>K2ABC</code> structure.</p><p><strong>Example</strong></p><pre><code class="language-julia">using Distributions
model(x) = [randn() .+ x for _ in 1:3]
data = [[3.], [2.9], [3.3]]
k = K2ABC(kernel = Kernel(),
          epsilon = .1,
          prior = MultivariateNormal([0.], [5.]),
          K = 10^3)
result = run!(k, model, data)
mean(k)</code></pre><p><a href="http://proceedings.mlr.press/v51/park16.html">Park, M., Jitkrittum, W., and Sejdinovic, D. (2016), K2-ABC: Approximate Bayesian Computation with Kernel Embeddings, Proceedings of Machine Learning Research, 51:398-407 </a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbrea/LikelihoodfreeInference.jl/blob/679e229bcc3b04fe902332c2054fbb8b9908670f/src/kernelabc.jl#L17-L38">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LikelihoodfreeInference.KernelABC-Tuple{}" href="#LikelihoodfreeInference.KernelABC-Tuple{}"><code>LikelihoodfreeInference.KernelABC</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">KernelABC(; kernel, prior, K, delta)</code></pre><p>Creates a <code>KernelABC</code> structure.</p><p><strong>Example</strong></p><pre><code class="language-julia">using Distributions
model(x) = [randn() .+ x for _ in 1:3]
data = [[3.], [2.9], [3.3]]
k = KernelABC(kernel = Kernel(),
              delta = 1e-8,
              prior = MultivariateNormal([0.], [5.]),
              K = 10^3)
result = run!(k, model, data)
mean(k)</code></pre><p>Fukumizu, K., Song, L. and Gretton, A. (2013), Kernel Bayes&#39; Rule: Bayesian Inference with Positive Definite Kernels, Journal of Machine Learning Research, 14:3753-3783, http://jmlr.org/papers/v14/fukumizu13a.html</p><p>See also <a href="http://dx.doi.org/10.1561/2200000060">Muandet, K., Fukumizu, K., Sriperumbudur, B. and Schölkopf, B. (2017), Kernel Mean Embedding of Distributions: A Review and Beyond, Foundations and Trends® in Machine Learning, 10:1–141 </a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbrea/LikelihoodfreeInference.jl/blob/679e229bcc3b04fe902332c2054fbb8b9908670f/src/kernelabc.jl#L78-L105">source</a></section></article><h2 id="Approximate-Point-Estimators-1"><a class="docs-heading-anchor" href="#Approximate-Point-Estimators-1">Approximate Point Estimators</a><a class="docs-heading-anchor-permalink" href="#Approximate-Point-Estimators-1" title="Permalink"></a></h2><p>For possible optimizers for <code>PointEstimator</code> see <a href="#(Optional)-Optimizers-for-PointEstimator-1">(Optional) Optimizers for PointEstimator</a>.</p><article class="docstring"><header><a class="docstring-binding" id="LikelihoodfreeInference.KernelLoss" href="#LikelihoodfreeInference.KernelLoss"><code>LikelihoodfreeInference.KernelLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">KernelLoss(; loss = L, K = 50,
             kerneltype = ModifiedGaussian,
             gamma = .1,
             heuristic = Nothing,
             prior = nothing,
             distance = euclidean,
             kernel = Kernel(type = kerneltype,
                             heuristic = heuristic,
                             gamma = gamma,
                             distance = distance))</code></pre><p>Constructs an structure, used to compare <code>K</code> samples generated from a model to some data with a <code>kernel</code> and loss function <code>loss</code>. The loss function has the signature <code>L(list_of_samples, data, kernel)</code>; by default it is the negative log of the average distance between samples and data (see <a href="@ref">LikelihoodfreeInference.L</a>). The <code>KernelLoss</code> can be used as a loss in a <a href="#LikelihoodfreeInference.PointEstimator"><code>PointEstimator</code></a>.</p><p>The returned structure is callable.</p><p><strong>Example</strong></p><pre><code class="language-julia">model(x) = randn() .+ x
data = [2.]
k = KernelLoss()
k([1.], model, data)</code></pre><p><code>K</code> can be an integer or a function that returns integers, to implement a schedule.</p><p><strong>Example</strong></p><pre><code class="language-julia">using Statistics
model(x) = randn() .+ x
data = [2.]
k = KernelLoss(K = (n) -&gt; n &gt; 20 ? 10^3 : 10)
std(k([1.], model, data, 1) for _ in 1:100)   # large std, because K = 10
std(k([1.], model, data, 21) for _ in 1:100)  # small std, because K = 10^3</code></pre><p>This <code>KernelLoss</code> is inspired by <a href="http://dx.doi.org/10.1515/sagmb-2017-0016">Bertl, J., Ewing, G., Kosiol, C., and Futschik, A., (2017) Approximate maximum likelihood estimation for population genetic inference, Statistical Applications in Genetics and Molecular Biology. 16:5-6 </a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbrea/LikelihoodfreeInference.jl/blob/679e229bcc3b04fe902332c2054fbb8b9908670f/src/pointestimator.jl#L5-L49">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LikelihoodfreeInference.PointEstimator" href="#LikelihoodfreeInference.PointEstimator"><code>LikelihoodfreeInference.PointEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">PointEstimator(optimizer, loss)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbrea/LikelihoodfreeInference.jl/blob/679e229bcc3b04fe902332c2054fbb8b9908670f/src/pointestimator.jl#L149-L151">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LikelihoodfreeInference.PointEstimator-Tuple{}" href="#LikelihoodfreeInference.PointEstimator-Tuple{}"><code>LikelihoodfreeInference.PointEstimator</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">PointEstimator(; prior = nothing, lower = nothing, upper = nothing,
                 optimizer = nlopt(prior),
                 losstype = QDLoss, kwargs...)</code></pre><p>Creates a <code>PointEstimator</code> with given <code>prior</code>, <code>optimizer</code> and <code>loss = losstype(; prior = prior, kwargs...)</code>. If the prior is <code>nothing</code> a uniform prior is assumed. In this case the optimizer has to be initialized with bounds, , e.g. <code>nlopt(lower, upper)</code> where <code>lower</code> and <code>upper</code> are arrays.</p><p><strong>Example</strong></p><pre><code class="language-julia">model(x) = randn() .+ x
data = [2.]
p = PointEstimator(lower = [-5], upper = [5], losstype = QDLoss, K = 10^3)
res = run!(p, model, data, maxfevals = 10^5)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbrea/LikelihoodfreeInference.jl/blob/679e229bcc3b04fe902332c2054fbb8b9908670f/src/pointestimator.jl#L156-L173">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LikelihoodfreeInference.QDLoss-Tuple{}" href="#LikelihoodfreeInference.QDLoss-Tuple{}"><code>LikelihoodfreeInference.QDLoss</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">QDLoss(; K = 50, epsilon = 2/K, prior = nothing, distance = euclidean)</code></pre><p>Constructs an structure, used to compare <code>K</code> samples generated from a model to some data by computing the first epsilon-quantile of the <code>distance</code> between samples and data. The <code>QDLoss</code> can be used as a loss in a <a href="#LikelihoodfreeInference.PointEstimator"><code>PointEstimator</code></a>.</p><p>The returned structure is callable.</p><p><strong>Example</strong></p><pre><code class="language-julia">model(x) = randn() .+ x
data = [2.]
q = QDLoss()
q([1.], model, data)</code></pre><p><code>K</code> can be an integer or a function that returns integers, to implement a schedule. Similarly, <code>epsilon</code> can be a number or a function that returns numbers.</p><p><strong>Example</strong></p><pre><code class="language-julia">using Statistics
model(x) = randn() .+ x
data = [2.]
q = QDLoss(K = (n) -&gt; n &gt; 20 ? 10^3 : 10)
std(q([1.], model, data, 1) for _ in 1:100)   # large std, because K = 10
std(q([1.], model, data, 21) for _ in 1:100)  # small std, because K = 10^3</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbrea/LikelihoodfreeInference.jl/blob/679e229bcc3b04fe902332c2054fbb8b9908670f/src/pointestimator.jl#L82-L110">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LikelihoodfreeInference.KernelRecursiveABC-Tuple{}" href="#LikelihoodfreeInference.KernelRecursiveABC-Tuple{}"><code>LikelihoodfreeInference.KernelRecursiveABC</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">KernelRecursiveABC(; kernel, prior, K, delta = 1e-2,
                     herding_options = (maxtime = 10,
                                        method = :LD_LBFGS,
                                        restarts = 10),
                     kernelx = Kernel())</code></pre><p>Creates a <code>KernelRecursiveABC</code> structure. <code>KernelRecursiveABC</code> iterates a <a href="#LikelihoodfreeInference.KernelABC-Tuple{}"><code>KernelABC</code></a> step and a kernel herding step. <code>kernel</code>, <code>prior</code>, <code>K</code> and <code>delta</code> determine the <code>KernelABC</code> step. <code>herding_options</code> options and <code>kernelx</code> determine the herding step.</p><p><strong>Example</strong></p><pre><code class="language-julia">using Distributions
model(x) = [randn() .+ x for _ in 1:3]
data = [[3.], [2.9], [3.3]]
k = KernelRecursiveABC(kernel = Kernel(),
                       delta = 1e-8,
                       prior = MultivariateNormal([0.], [5.]),
                       K = 10^2)
result = run!(k, model, data, maxfevals = 10^3)
result.x</code></pre><p><a href="http://proceedings.mlr.press/v80/kajihara18a.html">Kajihara, T., Kanagawa, M., Yamazaki, K. and Fukumizu, K. (2018), Kernel Recursive ABC: Point Estimation with Intractable Likelihood, Proceedings of the 35th International Conference on Machine Learning, 80:2400-2409 </a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbrea/LikelihoodfreeInference.jl/blob/679e229bcc3b04fe902332c2054fbb8b9908670f/src/krabc.jl#L7-L36">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LikelihoodfreeInference.L-Tuple{Any,Any,Any}" href="#LikelihoodfreeInference.L-Tuple{Any,Any,Any}"><code>LikelihoodfreeInference.L</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">L(x, y, k) = -log(mean(k(xi, y) for xi in x))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbrea/LikelihoodfreeInference.jl/blob/679e229bcc3b04fe902332c2054fbb8b9908670f/src/pointestimator.jl#L1-L3">source</a></section></article><h2 id="Additional-Distributions-1"><a class="docs-heading-anchor" href="#Additional-Distributions-1">Additional Distributions</a><a class="docs-heading-anchor-permalink" href="#Additional-Distributions-1" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LikelihoodfreeInference.MultiParticleNormal" href="#LikelihoodfreeInference.MultiParticleNormal"><code>LikelihoodfreeInference.MultiParticleNormal</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MultiParticleNormal(particles, weights;
                    lower = nothing, upper = nothing,
                    diagonal = false,
                    scaling = 2, regularization = 0.)</code></pre><p>Mixture of <code>k</code> multivariate Gaussians with means given by an array of <code>particles</code> (<code>length(particles) == k</code>), weighted by <code>weights</code> and with covariance matrix given by <code>scaling * cov(particles) + regularization * I</code> for each Gaussian, if <code>diagonal == false</code>; otherwise <code>scaling * std(particles) + regularization</code>. Sampling truncated Gaussians with diagonal covariance matrix can be more efficient. The mean of the distribution is the weighted average of the particles.</p><p>If <code>lower</code> and <code>upper</code> are not <code>nothing</code> the Gaussians are truncated to the box confined by the <code>lower</code> and <code>upper</code> bounds. Note that the logpdf of the truncated version is not properly normalized (see also <a href="#LikelihoodfreeInference.TruncatedMultivariateNormal"><code>TruncatedMultivariateNormal</code></a>).</p><p><strong>Example</strong></p><pre><code class="language-julia-repl">julia&gt; d = MultiParticleNormal([[0, 0], [2, 3], [4, 0]], [1/3, 1/3, 1/3]);

julia&gt; mean(d)
2-element Array{Float64,1}:
 2.0
 1.0</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbrea/LikelihoodfreeInference.jl/blob/679e229bcc3b04fe902332c2054fbb8b9908670f/src/distributions.jl#L120-L146">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LikelihoodfreeInference.MultiParticleNormal-Tuple{}" href="#LikelihoodfreeInference.MultiParticleNormal-Tuple{}"><code>LikelihoodfreeInference.MultiParticleNormal</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">MultiParticleNormal(; kwargs...)</code></pre><p>Returns a anonymous function that accepts <code>particles</code> as input and creates a <code>MultiParticleNormal</code> distributions with uniform weights. It accepts the same keyword arguments <code>kwargs...</code> as the normal <code>MultiParticleNormal</code> constructor.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl">julia&gt; constructor = MultiParticleNormal(scaling = 3, lower = [-1, 0], upper = [5, 5]);

julia&gt; d = constructor([[0, 0], [2, 3], [4, 0], [5, 1]])
MultiParticleNormal(4 particles in 2 dimensions, scaling = 3.0, regularization = 0.0, with bounds)

julia&gt; d.weights
4-element Array{Float64,1}:
 0.25
 0.25
 0.25
 0.25</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbrea/LikelihoodfreeInference.jl/blob/679e229bcc3b04fe902332c2054fbb8b9908670f/src/distributions.jl#L156-L176">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LikelihoodfreeInference.MultivariateUniform" href="#LikelihoodfreeInference.MultivariateUniform"><code>LikelihoodfreeInference.MultivariateUniform</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MultivariateUniform(lower, upper)</code></pre><p>Uniform distribution in <code>d</code> dimensions, where <code>d == length(lower) == length(upper)</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl">julia&gt; d = MultivariateUniform([-2., -3], [4, 1])
MultivariateUniform{Float64} in 2 dimensions

julia&gt; logpdf(d, [-3, 0])
-Inf

julia&gt; logpdf(d, [-1, 0])
-3.1780538303479453</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbrea/LikelihoodfreeInference.jl/blob/679e229bcc3b04fe902332c2054fbb8b9908670f/src/distributions.jl#L1-L16">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LikelihoodfreeInference.TruncatedMultivariateNormal" href="#LikelihoodfreeInference.TruncatedMultivariateNormal"><code>LikelihoodfreeInference.TruncatedMultivariateNormal</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">TruncatedMultivariateNormal(mvnormal, lower, upper)</code></pre><p>Multivariate normal distribution <code>mvnormal</code> truncated to a box given by <code>lower</code> and <code>upper</code> bounds. Simple rejection sampling is implemented. IMPORTANT: <code>logpdf</code> is not properly normalized.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl">julia&gt; using Distributions

julia&gt; d = TruncatedMultivariateNormal(MultivariateNormal([2., 3.], .3*I), [0, 0], [4, 5]);

julia&gt; logpdf(d, [2, 3])
-0.6339042620834094

julia&gt; logpdf(d, [2, 3]) == logpdf(d.mvnormal, [2, 3])
true

julia&gt; logpdf(d, [-1, 1])
-Inf</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbrea/LikelihoodfreeInference.jl/blob/679e229bcc3b04fe902332c2054fbb8b9908670f/src/distributions.jl#L50-L72">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LikelihoodfreeInference.TruncatedMultivariateNormal-Tuple{Any,Any}" href="#LikelihoodfreeInference.TruncatedMultivariateNormal-Tuple{Any,Any}"><code>LikelihoodfreeInference.TruncatedMultivariateNormal</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">TruncatedMultivariateNormal(m, cov; lower, upper)</code></pre><p>Constructs a <code>TruncatedMultivariateNormal</code> with mean <code>m</code> covariance matrix <code>cov</code> and bounds <code>lower</code> and <code>upper</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl">julia&gt; d = TruncatedMultivariateNormal([0, 0], 3I;
                                       lower = [-1, -4], upper = [4, 5])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbrea/LikelihoodfreeInference.jl/blob/679e229bcc3b04fe902332c2054fbb8b9908670f/src/distributions.jl#L78-L88">source</a></section></article><h2 id="(Optional)-Optimizers-for-PointEstimator-1"><a class="docs-heading-anchor" href="#(Optional)-Optimizers-for-PointEstimator-1">(Optional) Optimizers for PointEstimator</a><a class="docs-heading-anchor-permalink" href="#(Optional)-Optimizers-for-PointEstimator-1" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LikelihoodfreeInference.bbo-Tuple{Any,Any}" href="#LikelihoodfreeInference.bbo-Tuple{Any,Any}"><code>LikelihoodfreeInference.bbo</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">bbo(lower, upper; kwargs...)
bbo(prior; kwargs...) = bbo(lower(prior), upper(prior); kwargs...)</code></pre><p>BlackBoxOptim optimizer for <a href="#LikelihoodfreeInference.PointEstimator"><code>PointEstimator</code></a>, available when <code>using BlackBoxOptim</code>. <code>kwargs</code> are passed to <code>BlackBoxOptim.bboptimize</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbrea/LikelihoodfreeInference.jl/blob/679e229bcc3b04fe902332c2054fbb8b9908670f/src/optional_optimization.jl#L53-L59">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LikelihoodfreeInference.bo-Tuple{Any,Any}" href="#LikelihoodfreeInference.bo-Tuple{Any,Any}"><code>LikelihoodfreeInference.bo</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">bo(lower, upper;
        acquisition = BO.UpperConfidenceBound(),
        capacity = 10^3,
        mean = BO.GaussianProcesses.MeanConst(0.),
        kernel = BO.GaussianProcesses.SEArd(zeros(length(lower)), 5.),
        logNoise = 0,
        modeloptimizer_options = NamedTuple(),
        kwargs...)
bo(prior; kwargs...) = bo(lower(prior), upper(prior); kwargs...)</code></pre><p>BayesianOptimization optimizer for <a href="#LikelihoodfreeInference.PointEstimator"><code>PointEstimator</code></a>, available when <code>using BayesianOptimization</code>. <code>kwargs</code> are passed to <code>BayesianOptimization.BOpt</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbrea/LikelihoodfreeInference.jl/blob/679e229bcc3b04fe902332c2054fbb8b9908670f/src/optional_optimization.jl#L88-L101">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LikelihoodfreeInference.cma-Tuple{Any,Any}" href="#LikelihoodfreeInference.cma-Tuple{Any,Any}"><code>LikelihoodfreeInference.cma</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">cma(lower, upper;
    sigma0 = 0.25,
    fminoptions = NamedTuple(),
    filename = &quot;tmp-&quot;,
    kwargs...)
 cma(prior; kwargs...) = cma(lower(prior), upper(prior); kwargs...)</code></pre><p>CMA optimizer for <a href="#LikelihoodfreeInference.PointEstimator"><code>PointEstimator</code></a>, available when <code>using PyCMA</code>. <code>kwargs</code> are passed to <code>PyCMA.cma.CMAOptions</code>, <code>fminoptions</code> to <code>PyCMA.cma.fmin</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbrea/LikelihoodfreeInference.jl/blob/679e229bcc3b04fe902332c2054fbb8b9908670f/src/optional_optimization.jl#L9-L19">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LikelihoodfreeInference.spsa-Tuple{Any,Any}" href="#LikelihoodfreeInference.spsa-Tuple{Any,Any}"><code>LikelihoodfreeInference.spsa</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">spsa(lower, upper; kwargs...)
spsa(prior; kwargs...) = spsa(lower(prior), upper(prior); kwargs...)</code></pre><p>SimultaneousPerturbationStochasticApproximation optimizer for <a href="#LikelihoodfreeInference.PointEstimator"><code>PointEstimator</code></a>, available when <code>using SimultaneousPerturbationStochasticApproximation</code>. <code>kwargs</code> are passed to <code>SimultaneousPerturbationStochasticApproximation.SPSA</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbrea/LikelihoodfreeInference.jl/blob/679e229bcc3b04fe902332c2054fbb8b9908670f/src/optional_optimization.jl#L143-L148">source</a></section></article><h2 id="Utilities-1"><a class="docs-heading-anchor" href="#Utilities-1">Utilities</a><a class="docs-heading-anchor-permalink" href="#Utilities-1" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LikelihoodfreeInference.EpsilonExponentialDecay" href="#LikelihoodfreeInference.EpsilonExponentialDecay"><code>LikelihoodfreeInference.EpsilonExponentialDecay</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">EpsilonExponentialDecay(init, last, decay)</code></pre><p>Exponentially decreasing sequence starting at <code>init</code>, decaying with factor <code>decay</code>, ending as soon as it equals (or drops below) <code>last</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl">julia&gt; collect(EpsilonExponentialDecay(1, .2, .5))
4-element Array{Any,1}:
 1.0
 0.5
 0.25
 0.125</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbrea/LikelihoodfreeInference.jl/blob/679e229bcc3b04fe902332c2054fbb8b9908670f/src/utils.jl#L88-L102">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LikelihoodfreeInference.EpsilonLinearDecay" href="#LikelihoodfreeInference.EpsilonLinearDecay"><code>LikelihoodfreeInference.EpsilonLinearDecay</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">EpsilonLinearDecay(init, last, decay)</code></pre><p>Linearly decreasing sequence starting at <code>init</code>, decaying with step size <code>decay</code>, ending as soon as it equals (or drops below) <code>last</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl">julia&gt; collect(EpsilonLinearDecay(10, 5, 2))
4-element Array{Any,1}:
 10.0
  8.0
  6.0
  4.0</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbrea/LikelihoodfreeInference.jl/blob/679e229bcc3b04fe902332c2054fbb8b9908670f/src/utils.jl#L117-L131">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LikelihoodfreeInference.defaultproposal-Tuple{Any}" href="#LikelihoodfreeInference.defaultproposal-Tuple{Any}"><code>LikelihoodfreeInference.defaultproposal</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">defaultproposal(prior)</code></pre><p>Returns a function that takes a list of <code>particles</code> and returns a proposal distribution. This function is called during initialization of <a href="#LikelihoodfreeInference.PMC"><code>PMC</code></a> and <a href="#LikelihoodfreeInference.AdaptiveSMC"><code>AdaptiveSMC</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbrea/LikelihoodfreeInference.jl/blob/679e229bcc3b04fe902332c2054fbb8b9908670f/src/utils.jl#L64-L70">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../generated/blowfly/">« Example: Blowfly Model</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Wednesday 8 April 2020 07:51">Wednesday 8 April 2020</span>. Using Julia version 1.3.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
